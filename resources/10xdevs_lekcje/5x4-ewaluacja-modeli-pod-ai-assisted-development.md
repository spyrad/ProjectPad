---
title: "[5x4] Ewaluacja modeli pod AI-Assisted Development"
course: "10xdevs-2"
source: "Przeprogramowani.pl"
exported: "2025-11-09"
format: "markdown"
---

![Ilustracja z lekcji](https://assets-v2.circle.so/lckrw4m07nomllcriwo4qndr4ies)

## Wprowadzenie

Jak rzetelnie sprawdzaÄ‡, ktÃ³ry model jÄ™zykowy naprawdÄ™ przyspieszy i uÅ‚atwi twojÄ… pracÄ™? MoÅ¼esz to robiÄ‡ poÅ›rednio poprzez codziennÄ… praktykÄ™, ale przy duÅ¼ej skali i czÄ™stej zmiennoÅ›ci modeli trudno o nadÄ…Å¼anie za kaÅ¼dÄ… nowÄ… opcjÄ….

W tej lekcji poznasz rekomendowane przez nas narzÄ™dzie do testowania i porÃ³wnywania rÃ³Å¼nych modeli jÄ™zykowych, optymalizacji promptÃ³w oraz wiarygodnej oceny tego, ktÃ³ry model najlepiej sprawdzi siÄ™ w konkretnych zadaniach programistycznych. Nie bÄ™dÄ… to jednak suche testy i akademickie benchmarki, a uÅ¼yteczny framework i narzÄ™dzie do oceny LLMÃ³w pod kÄ…tem ich zdolnoÅ›ci w preferowanym przez ciebie stacku technologicznym.

Zanim skupimy siÄ™ na konkretnym przykÅ‚adzie, najpierw przedstawimy ogÃ³lne zaÅ‚oÅ¼enia i najwaÅ¼niejsze zagadnienia ze Å›wiata testowania modeli.

## Evals, czyli ewaluacja modeli AI

NiezaleÅ¼nie od konkretnego narzÄ™dzia czy platformy, tzw. â€œ_evale_â€ (testy modeli) sprowadzajÄ… siÄ™ do kilku ustandaryzowanych praktyk.

Podstawowa forma testu bÄ™dzie zawieraÄ‡ nastÄ™pujÄ…ce elementy:

- **Dane wejÅ›ciowe**  
   - Prompt lub Å‚aÅ„cuch promptÃ³w wejÅ›ciowych
- **Scenariusz testu**  
   - Oczekiwana poprawna odpowiedÅº lub kontynuacja konwersacji przez model (_ground truth_)  
   - Typ asercji (np. tekstowe _equals, contains, not contains, itd._)
- **Przedmiot testu**  
   - OdpowiedÅº testowanego modelu

W poniÅ¼szym przykÅ‚adzie trzykrotnie testujemy zachowanie tego samego modelu i sprawdzamy, czy rozumie zarÃ³wno pytanie jak i oczekiwania wzglÄ™dem formatu odpowiedzi:

![Ilustracja z lekcji](https://assets-v2.circle.so/b263328c8cyt78wbsj7enrnz2u4q)

Po trzykrotnym uruchomieniu modelu z tymi samymi parametrami, uzyskujemy 33% poprawnych odpowiedzi. 

Aby uzyskaÄ‡ realne wyobraÅ¼enie o jakoÅ›ci danego modelu, najczÄ™Å›ciej bÄ™dziemy wykorzystywaÄ‡ nie jeden, a kilka testÃ³w (_batch_) wpÅ‚ywajÄ…cych na caÅ‚e badanie. Aby badanie byÅ‚o miarodajne, prawdopodobnie chcielibyÅ›my zwiÄ™kszyÄ‡ skalÄ™ co najmniej o 50-100x.

### Wariantowanie promptÃ³w

Wiele narzÄ™dzi do testowania modeli wspiera dodatkowy wymiar testÃ³w, czyli parametryzacjÄ™ samego prompta wejÅ›ciowego.

OsiÄ…gamy to poprzez:

- wprowadzenie zmiennych do prompta, np. {{country}}
- wykonanie testu na okreÅ›lonym zestawie danych testowych (tzw. dataset)

Rozszerzony scenariusz prezentuje poniÅ¼szy schemat:

![Ilustracja z lekcji](https://assets-v2.circle.so/j6eihn1ubmvwhxqnywq1neck0aay)

W takim wariancie nasze badanie wykona siÄ™ wg rÃ³wnania:

> **_1 prompt_** x **_liczba wierszy z datasetu_** x **_liczba powtÃ³rzeÅ„ danego scenariusza_**

DziÄ™ki przetestowaniu 1 prompta dla 3 paÅ„stw po 3 razy uzyskujemy 9 odpowiedzi z modelu.

Parametryzowanie promptÃ³w pozwala nam uzyskaÄ‡ znacznie lepszy obraz tego, jak wybrany model zachowuje siÄ™ dla rÃ³Å¼nych danych wejÅ›ciowych i dla rÃ³Å¼nego zestawu danych oczekiwanych.

Co waÅ¼ne, same testy nie muszÄ… dziaÅ‚aÄ‡ wyÅ‚Ä…cznie z warunkiem typu â€œ_equals_â€, ale mogÄ… teÅ¼ oceniaÄ‡ modele pod kÄ…tem wÅ‚aÅ›ciwej klasyfikacji tekstu, zrozumienia poleceÅ„, blokowania okreÅ›lonych zachowaÅ„ czy unikania odpowiedzi na podchwytliwe pytania. Wszystko sprowadza siÄ™ jednak do podobnych elementÃ³w - promptu wejÅ›ciowego, odpowiedzi oczekiwanej oraz odpowiedzi generowanej przez model w trakcie testu (do oceny).

Testowanie w tym stylu wspierajÄ… choÄ‡by:

- [OpenAI - Evals na platformie oraz w API](https://platform.openai.com/docs/guides/evals)
- [Anthropic - Evals w Anthropic Console](https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool)

Obszerne teoretyczne omÃ³wienie caÅ‚ego tematu â€œevaliâ€ znajdziesz w otwartym repozytorium od HuggingFace:

<https://github.com/huggingface/evaluation-guidebook>

## Evals vs. AI-Assisted Development

Przedstawione powyÅ¼ej informacje sprawdzÄ… siÄ™ dobrze w integracjach bazujÄ…cych na tekÅ›cie (np. planowanie projektu, klasyfikacja, konwersacje z AI), ale nie dajÄ… nam informacji o tym, czy odpowiedÅº moÅ¼e byÄ‡ faktycznie wykorzystywana do programowaniu w okreÅ›lonym stacku.

Miarodajne badanie powinno oceniaÄ‡ m.in. to, czy model:

- potrafi pisaÄ‡ kod w okreÅ›lonym jÄ™zyku programowania
- czy rozumie polecenia i speÅ‚nia zaÅ‚oÅ¼enia dot. logiki biznesowej
- czy kod przechodzi okreÅ›lone testy (np. jednostkowe)
- czy kod zawiera okreÅ›lone standardy lub wzorce (np. let/const zamiast var w JavaScript)
- czy kod speÅ‚nia wymagania dot. wydajnoÅ›ci (np. unika zagnieÅ¼dÅ¼onych pÄ™tli)
- itd.

W tym celu musimy wykorzystaÄ‡ narzÄ™dzie, ktÃ³re nie tylko jest w stanie komunikowaÄ‡ siÄ™ z modelem i oceniaÄ‡ jego odpowiedzi w formie surowego tekstu, ale takie, ktÃ³ry ten tekst konwertuje na rzeczywisty kod poddawany dalszej ocenie.

Jednym z takich narzÄ™dzi jest [Promptfoo](https://www.promptfoo.dev/), ktÃ³re moÅ¼na zintegrowaÄ‡ z istniejÄ…cym projektem w dowolnym stacku, a nastÄ™pnie przy jego pomocy wykonywaÄ‡ ocenÄ™ jakoÅ›ci odpowiedzi z AI.

## Promptfoo w akcji

Na dobry poczÄ…tek zapoznaj siÄ™ z filmem, w ktÃ³rym testujemy trzy rÃ³Å¼ne modele pod kÄ…tem znajomoÅ›ci skÅ‚adni frameworka Svelte 5.

Obejrzyj ogÃ³lne zaÅ‚oÅ¼enia, a w dalszej czÄ™Å›ci przejdziemy przez poszczegÃ³lne etapy testowania:

ğŸ¥ **VIDEO**: [Watch here](https://player.vimeo.com/video/1079142959?app_id=122963&byline=0&badge=0&portrait=0&title=0)

Jak Å‚atwo zauwaÅ¼yÄ‡, taka forma testowania modeli przynosi istotne korzyÅ›ci wzglÄ™dem rÄ™cznego wywoÅ‚ywania setek promptÃ³w na wielu rÃ³Å¼nych platformach.

Promptfoo umoÅ¼liwia Å‚atwe porÃ³wnywanie wielu modeli wzglÄ™dem siebie, a ten sam prompt wejÅ›ciowy moÅ¼e byÄ‡ poddany rÃ³Å¼norodnym testom z odpowiednimi parametrami. Testy te mogÄ… zawieraÄ‡ dowolne asercje dotyczÄ…ce realnej oceny odpowiedzi i/lub kodu, a dodatkowo sÄ… uruchamiane rÃ³wnolegle, co przyÅ›piesza i automatyzuje caÅ‚y proces testowania.

WaÅ¼nym aspektem jest rÃ³wnieÅ¼ to, Å¼e caÅ‚oÅ›Ä‡ konfigurowana jest poprzez deklaratywne ustawienia w pliku YAML - ustawienia projektu testowego mogÄ… byÄ‡ Å›ledzone przez GITa i edytowane z zachowaniem peÅ‚nej kontroli zmian (nie musimy bazowaÄ‡ na dostawcy zewnÄ™trznej platformy typu SaaS).

Aby dobrze zrozumieÄ‡ dziaÅ‚anie caÅ‚ego rozwiÄ…zania, przejdÅºmy przez najwaÅ¼niejsze kroki caÅ‚ego procesu testowania z promptfoo.

## Pierwsze kroki

Promptfoo jest rozwiÄ…zaniem publikowanym w dwÃ³ch rejestrach:

- w **npm**: <https://www.npmjs.com/package/promptfoo>
- w **homebrew**: <https://formulae.brew.sh/formula/promptfoo>

NarzÄ™dzie moÅ¼na wykorzystywaÄ‡ na kilka sposobÃ³w - po instalacji jako globalny util, jako narzÄ™dzie uruchamiane poprzez _npx_, a takÅ¼e jako lokalnÄ… zaleÅ¼noÅ›Ä‡ w projekcie:

```
# globalnie z npm:
npm install -g promptfoo
# lub
npx promptfoo@latest
# lub w projekcie:
npm install promptfoo -DE
# lub z homebrew:
brew install promptfoo
```

Polecenie _init_ utworzy dla ciebie pierwszÄ… wersjÄ™ konfiguracji projektu (_promptfooconfig.yaml_):

```
npx promptfoo@latest init
# lub, po instalacji
promptfoo init

# ==> promptfooconfig.yaml
```

Sama konfiguracja jest naprawdÄ™ intuicyjna - na poziomie projektu moÅ¼emy definiowaÄ‡ zestaw dostawcÃ³w i wersji modeli, ich konfiguracjÄ™, testowe prompty, a takÅ¼e wybrane asercje:

```
providers:
  - id: openai:gpt-5
    label: gpt-5
    config:
      apiKey: ######
      temperature: 0.7
      max_tokens: 1000

  - id: anthropic:messages:claude-sonnet-4-5-20250929
    label: sonnet-4_5
    config:
      apiKey: ######
      temperature: 0.7
      max_tokens: 1000

prompts:
  - 'Translate the following text to Polish: "{{sentence}}". Respond only with the translation, no other text.'

tests:
  - vars:
      sentence: How are you?
    assert:
      - type: equals
        value: Jak siÄ™ masz?
  - vars:
      sentence: Wie gehts?
    assert:
      - type: equals
        value: Jak leci?

```

KonfiguracjÄ™ moÅ¼na uruchomiÄ‡ poleceniem â€œevalâ€, wskazujÄ…c dodatkowo na opcjonalny plik wynikowy.

```
promptfoo eval --output results.txt
```

W efekcie otrzymamy podsumowanie naszego testu:

![Ilustracja z lekcji](https://assets-v2.circle.so/w36mz7bz8b5a5c4yxh9w90ii9vxk)

NiewÄ…tpliwÄ… zaletÄ… promptfoo jest duÅ¼a liczba dostÄ™pnych modeli i samych integracji, w tym OpenRouter, OpenAI API, Anthropic API, Grok API, itd. - caÅ‚a listÄ™ znajdziesz [tutaj](https://www.promptfoo.dev/docs/providers/) (moÅ¼na rÃ³wnieÅ¼ budowaÄ‡ niestandardowych _providerÃ³w_ przy pomocy JavaScriptu i Pythona).

Modele mogÄ… byÄ‡ poddawane testom w wielu wymiarach - asercje mogÄ… zawieraÄ‡:

- deterministyczne kryteria jakoÅ›ci odpowiedzi, np. equals, contains, is-json, itd. - [wiÄ™cej tutaj](https://www.promptfoo.dev/docs/configuration/expected-outputs/deterministic/)
- LLM-as-a-Judge (wybrany model ocenia odpowiedÅº modelu testowanego) - [konfiguracja](https://www.promptfoo.dev/docs/configuration/expected-outputs/model-graded/#overriding-the-llm-grader)
- odwoÅ‚ania do zewnÄ™trznych plikÃ³w (wÅ‚asne skrypty Node/Python do oceny jakoÅ›ci odpowiedzi)

O ile pierwsze dwa typy asercji mogÄ… siÄ™ przydaÄ‡ chociaÅ¼by do oceny jakoÅ›ci planowania czy kreatywnej burzy mÃ³zgu prowadzonej przez AI, to ostatni element pozwoli nam rozszerzyÄ‡ moÅ¼liwoÅ›ci promptfoo pod AI-Assisted Development.

## Rozszerzanie moÅ¼liwoÅ›ci promptfoo

OgromnÄ… zaletÄ… promptfoo jest moÅ¼liwoÅ›Ä‡ zamiany promptÃ³w, testÃ³w i asercji tekstowych, na asynchroniczne funkcje pisane w jÄ™zykach JavaScript (CJS/ESM) oraz Python.

Najprostsza konfiguracja bÄ™dzie zawieraÄ‡ jeden plik z eksportowanymi funkcjami (wÅ‚aÅ›ciwa implementacja moÅ¼e zawieraÄ‡ dedykowane moduÅ‚y na prompty, testy i asercje):

```
// evalConfig.js

export async function prompt() {
  return 'Translate the following text to Polish: "{{sentence}}". Respond only with the translation, no other text.';
}

export async function testCases() {
  return [
    {
      id: "test-1",
      description: "Test #1",
      vars: {sentence: "How are you?"},
      assert: [
        {
          type: "javascript",
          value: async (output, context) => {

           // - Run vitest
           // - Run eslint
           // - Run ...

            const pass = output === "Jak siÄ™ masz?";
            return {
              pass,
              score: pass ? 1 : 0,
              reason: pass ? "Correct" : "Incorrect",
            };
          },
        },
      ],
    },
  ];
}

export async function teardownHook(hookName) {
  if (hookName === "afterAll") {
    // ... cleanup
  }
}
```

Do funkcji moÅ¼emy siÄ™ odwoÅ‚ywaÄ‡ poprzez Å›cieÅ¼kÄ™ do pliku oraz wskazanie konkretnej funkcji (jeÅ›li nie decydujemy siÄ™ na eksporty domyÅ›lne - wtedy nazwa funkcji jest zbÄ™dna):

```
# promptfooconfig.yaml

prompts:
  - file://evalConfig.js:prompt

tests:
  - file://evalConfig.js:testCases

extensions:
 - file://evalConfig.js:teardownHook
```

### Niestandardowy stack w promptfoo

ChociaÅ¼ promptfoo natywnie korzysta z rozszerzeÅ„ Pythona i JavaScriptu, w obu jÄ™zykach moÅ¼emy â€œzespawnowaÄ‡â€ zadania innego typu (np. testy jednostkowe .NET).

JavaScript:

```
// run-dotnet-xunit.mjs
import { execSync } from 'child_process';

const testProjectPath = './path/to/your/test/project';

try {
  console.log('Running .NET xUnit tests...');

  execSync(`dotnet test ${testProjectPath} --logger "xunit;LogFilePath=TestResults.xml"`, {
    stdio: 'inherit'
  });

  console.log('Tests completed successfully');
} catch (error) {
  console.error('Tests failed');
  process.exit(1);
}
```

**WaÅ¼ne:** PracujÄ…c z promptfoo w standardzie importÃ³w ESM, musiaÅ‚em ustawiÄ‡ na sztywno jednÄ… z wersji bibliotek, ktÃ³ra sprawiaÅ‚a problemy (downgrade z 3.x na 2.x):

```
"dependencies": {
    "estree-walker": "2.0.2"
 },
"overrides": {
    "estree-walker": "2.0.2"
  }
```

Python:

```
#!/usr/bin/env python3
# run_dotnet_xunit.py
import subprocess
import sys
from pathlib import Path

test_project_path = Path('./path/to/your/test/project')

print('Running .NET xUnit tests...')

try:
    subprocess.run(
        [
            'dotnet', 
            'test', 
            str(test_project_path),
            '--logger', 
            'xunit;LogFilePath=TestResults.xml'
        ],
        check=True, 
        text=True, 
        capture_output=False
    )

    print('Tests completed successfully')
except subprocess.CalledProcessError as e:
    print(f'Tests failed with exit code: {e.returncode}')
    sys.exit(e.returncode)
```

StÄ…d juÅ¼ prosta droga do prawdziwie skalowalnego, modularnego Å›rodowiska testÃ³w LLMÃ³w.

Przedstawiany na filmie projekt znajdziesz pod tym linkiem:

<https://github.com/przeprogramowani/10x-evals>

## Cache wynikÃ³w testÃ³w

Z punktu widzenia budÅ¼etu na testy, promptfoo ma jeszcze jednÄ… istotnÄ… zaletÄ™ - kiedy dany fragment konfiguracji siÄ™ nie zmienia pomiÄ™dzy kolejnymi _test runami_, narzÄ™dzie wykorzystuje cache do szybkiego generowania zapisanych odpowiedzi.

Klucz cacheâ€™a budowany jest wg wzorca:

> {dostawca modelu + konfiguracja modelu + zawartoÅ›Ä‡ prompta + zmienne do prompta} => output

DopÃ³ki Å¼aden z czterech elementÃ³w wejÅ›ciowych siÄ™ nie zmienia, promptfoo bÄ™dzie reuÅ¼ywaÄ‡ wczeÅ›niejsze odpowiedzi, a my unikniemy zbÄ™dnych kosztÃ³w.

JeÅ›li chcesz tymczasowo wyÅ‚Ä…czyÄ‡ ten mechanizm, dodaj do polecenia _eval_ dodatkowy parametr:

```
promptfoo eval --no-cache
```

## Metodyczne wdraÅ¼anie evali

ChociaÅ¼ twÃ³rcy promptfoo skupiajÄ… siÄ™ w swojej dokumentacji przede wszystkim na integracjach technicznych w aplikacjach opartych na AI, to ich rekomendacje moÅ¼emy z Å‚atwoÅ›ciÄ… przeksztaÅ‚ciÄ‡ w praktyczny workflow dla 10xDeva wspÃ³Å‚pracujÄ…cego z Agentami AI.

### Trzy obszary oceny AI

Trzy kluczowe obszary, ktÃ³re pozwolÄ… nam budowaÄ‡ wartoÅ›ciowe Å›rodowisko testÃ³w przekÅ‚adajÄ…ce siÄ™ na docelowÄ… wspÃ³Å‚pracÄ™ z AI, to:

- **Development** \- ciÄ…gÅ‚y rozwÃ³j scenariuszy i promptÃ³w do wspÃ³Å‚pracy z AI (np. prompty z 10xDevs)
- **Evaluation** \- asercje do oceny przydatnoÅ›ci modelu w okreÅ›lonym zadaniu
- **Production** \- praktyczna ocena modeli, ktÃ³re wypadÅ‚y najlepiej w fazie testowania

Z [dokumentacji](https://www.promptfoo.dev/docs/intro/) promptfoo:

![Ilustracja z lekcji](https://assets-v2.circle.so/gnp8zfsee12zv4z1joegdibxslyz)

Jak pracowaÄ‡ w tych obszarach? Kluczem do sukcesu jest podejÅ›cie iteracyjne - zaczynamy od spiÄ™cia caÅ‚ego flow end-to-end, a dopiero potem rozszerzamy je o zaawansowane przypadki brzegowe:

1. **Zidentyfikuj scenariusz** \- wybierz wartoÅ›ciowy scenariusz wspÃ³Å‚pracy z AI
2. **UtwÃ³rz pierwszy prompt** \- zaprojektuj podstawowÄ… wersjÄ™ promptu rozwiÄ…zujÄ…cego problem
3. **Minimalne asercje** \- okreÅ›l 1-2 kluczowe testy weryfikujÄ…ce poprawnoÅ›Ä‡ odpowiedzi
4. **Zintegruj promptfoo** \- utwÃ³rz bazowy plik konfiguracyjny Å‚Ä…czÄ…cy prompt z asercjami
5. **Przetestuj na jednym modelu** \- uruchom testy na domyÅ›lnym modelu, aby sprawdziÄ‡ dziaÅ‚anie

### PrzykÅ‚ad:

1. **Zadanie:** Implementacja komponentÃ³w w React przez Agenta AI
2. **Prompt:** UtwÃ³rz komponent w React + Tailwind + TypeScript wg {{spec}} bazujÄ…c na {{rules}}
3. **Testy:** Asercje do oceny komponentu, wykorzystujÄ…ce np. vitest, eslint, rules of hooks, itd.
4. **Tooling:** Integracja promptfoo - konfiguracja dostawcy modelu, promptu i testu
5. **Modele:** Testy na preferowanych modelach (np. Gemini 2.5 Pro, Claude 3.5 Sonnet, itd.)

### W kierunku sandboxa duÅ¼ej skali

Po uruchomieniu podstawowego flow moÅ¼esz systematycznie rozszerzaÄ‡ caÅ‚y system tworzÄ…c skalowalne Å›rodowisko do oceny AI w najbardziej istotnych obszarach (dla ciebie, twojego zespoÅ‚u lub caÅ‚ej firmy):

- **Rozbuduj asercje** \- dodaj bardziej szczegÃ³Å‚owe testy sprawdzajÄ…ce specyficzne aspekty odpowiedzi - moÅ¼esz wspomagaÄ‡ siÄ™ asercjami deterministycznymi, kodem, a takÅ¼e LLM-as-a-Judge, gdzie polegasz na zdolnoÅ›ciach modeli AI.
- **WprowadÅº warianty promptÃ³w** \- testuj rÃ³Å¼ne sformuÅ‚owania i struktury tego samego zadania, aby uzyskaÄ‡ bardziej przekrojowe Å›rodowisko do oceny modeli. W przypadku implementacji kodu mogÄ… to byÄ‡ urozmaicone wymagania biznesowe lub przypadki brzegowe (tak jak w klasycznych testach).
- **Dodaj wiÄ™cej modeli** \- porÃ³wnaj wyniki miÄ™dzy rÃ³Å¼nymi modelami AI, ktÃ³re pozwolÄ… ci zauwaÅ¼yÄ‡ istotne rÃ³Å¼nice w kosztach, szybkoÅ›ci i jakoÅ›ci odpowiedzi. MoÅ¼esz korzystaÄ‡ z platform duÅ¼ych firm jak OpenAI, Anthropic czy Grok, a takÅ¼e uniwersalnego integratora (OpenRouter).

KaÅ¼da runda testÃ³w dostarczy ci cennych wnioskÃ³w, dziÄ™ki ktÃ³rym udoskonalisz prompty, dokonasz selekcji modeli i zbudujesz bardziej precyzyjne asercje. Z kaÅ¼dÄ… iteracjÄ… TwÃ³j system bÄ™dzie stawaÅ‚ siÄ™ coraz bardziej niezawodny, a ocena nowych modeli - szybsza i dokÅ‚adniejsza.

Ten stopniowy rozwÃ³j pozwali uniknÄ…Ä‡ przeciÄ…Å¼enia zbyt duÅ¼Ä… liczbÄ… zmiennych na poczÄ…tku, jednoczeÅ›nie budujÄ…c solidne fundamenty pod bardziej zaawansowane testy w przyszÅ‚oÅ›ci.

PamiÄ™taj rÃ³wnieÅ¼ o poprzednich lekcjach - znajÄ…c CI/CD takie jak GitHub Actions, testowanie z promptfoo moÅ¼esz przekonwertowaÄ‡ na cyklicznie uruchamiane scenariusze i raportowanie, do ktÃ³rego wglÄ…d mogÄ… mieÄ‡ pozostali czÅ‚onkowie zespoÅ‚u.

Czy brzmi to jak firmowy AI Leaderboard? Czemu nie!

## Evale modeli vs. praktyczne testy AI-toolingu

NaleÅ¼y pamiÄ™taÄ‡, Å¼e w zaleÅ¼noÅ›ci od konkretnego przypadku i scenariusza wspÃ³Å‚pracy z AI, nasze testy bÄ™dÄ… oddawaÄ‡ rzeczywisty stan danego modelu na rÃ³Å¼ne sposoby.

PrzykÅ‚adowo, kiedy budujemy integracje AI z CI/CD Å‚Ä…czÄ…c siÄ™ bezpoÅ›rednio z API danej platformy hostujÄ…cej modele, nie korzystamy z Å¼adnej dodatkowej warstwy poÅ›redniej. W tym przypadku evale bÄ™dÄ… **dobrym wskaÅºnikiem** przydatnoÅ›ci modeli do budowanej funkcjonalnoÅ›ci, bo nasze prompty moÅ¼emy w 100% wspÃ³Å‚dzieliÄ‡ miÄ™dzy testami a praktykÄ….

Inaczej wyglÄ…da to w przypadku edytorÃ³w czy pluginÃ³w, ktÃ³re dostarczajÄ… wÅ‚asny backend lub realizujÄ… za kulisami prompt engineering.

Tutaj evale bÄ™dÄ… **czÄ™Å›ciowym wskaÅºnikiem** przydatnoÅ›ci modeli - pokaÅ¼Ä… istotne rÃ³Å¼nice miÄ™dzy poszczegÃ³lnymi generacjami (np. GPT-3.5 â†’ GPT-4 â†’ GPT-5), ale w niektÃ³rych przypadkach mogÄ… nie oddawaÄ‡ jakoÅ›ci wspÃ³Å‚pracy np. z poziomu Cursora. Na to bÄ™dzie skÅ‚adaÄ‡ siÄ™:

- iloÅ›Ä‡ wbudowanego, ukrytego promptingu, ktÃ³ry rozbudowuje nasze polecenia
- jakoÅ›Ä‡ integracji narzÄ™dzi z wybranymi modelami
- dostÄ™p do ÅºrÃ³deÅ‚ zewnÄ™trznych i umiejÄ™tnoÅ›Ä‡ ich przeszukiwania ad-hoc
- poziom kompresowania kontekstu, wprowadzanego np. w Cursorze dla optymalizacji kosztÃ³w
![Ilustracja z lekcji](https://assets-v2.circle.so/00dee9aybl248grwpz0789kxfsk4)

Czym w takim wypadku evale sÄ… bezuÅ¼yteczne? W Å¼adnym wypadku - zwykle z duÅ¼ym prawdopodobieÅ„stwem potwierdzÄ… lub wykluczÄ…:

- znajomoÅ›Ä‡ wybranego API, jÄ™zyka programowania lub technologii (wiedza w modelu)
- ostatniÄ… znanÄ… wersjÄ™ wykorzystywanej technologii (np. rozmowy o .NET 9, React 19, itd.)
- jakoÅ›Ä‡ komunikacji w wybranym jÄ™zyku naturalnym (polskim, angielskim, itd.)
- testowanie znajomoÅ›ci praktyk i konceptÃ³w programistycznych (np. wzorzec X w jÄ™zyku Y)
- jakoÅ›ciowÄ… kontynuacjÄ™ Å‚aÅ„cucha promptÃ³w (np. wyszukujÄ…c luki w analizie wymagaÅ„)

BazujÄ…c na slajdzie z preworku (poniÅ¼ej) moÅ¼na powiedzieÄ‡, Å¼e im wiÄ™cej warstw poÅ›rednich miÄ™dzy naszym poleceniem a platformÄ… AI, tym wiÄ™cej potencjalnych rÃ³Å¼nic miÄ™dzy evalem a praktykÄ… (zarÃ³wno na plus jak i minus - po prostu trudno to przewidzieÄ‡ dziÄ™ki samym testom).

## ğŸ Podsumowanie lekcji

Åšwiat sztucznej inteligencji poddaje nas nieustannej presji testowania nowych modeli, technik promptowania oraz warunkÃ³w dostawcÃ³w, ktÃ³rzy rywalizujÄ… na koszty i szybkoÅ›Ä‡ dziaÅ‚ania poszczegÃ³lnych systemÃ³w.

Aby trzymaÄ‡ rÄ™kÄ™ na pulsie i rozpoznawaÄ‡ optymalne dla nas rozwiÄ…zania w duÅ¼ej skali, nie moÅ¼emy polegaÄ‡ wyÅ‚Ä…cznie na testowaniu rÄ™cznym, z poziomu edytora czy konsoli dostawcy. W tym kontekÅ›cie polecanym przez nas rozwiÄ…zaniem bÄ™dzie _promptfoo_ \- narzÄ™dzie pozwoli porÃ³wnywaÄ‡ odpowiedzi setek dostÄ™pnych modeli poprzez deklaratywnÄ… konfiguracjÄ™ YAML a takÅ¼e zewnÄ™trzne skrypty, gdzie umieÅ›cimy preferowane przez nas kryteria oceny.

Promptfoo najlepiej czuje siÄ™ w Å›rodowisku JavaScript/Python, ale oba jÄ™zyki mogÄ… wywoÅ‚ywaÄ‡ zewnÄ™trzne, dostÄ™pne w ramach OS-a narzÄ™dzia z innego stacku. A stÄ…d prosta droga do testowania sprawnoÅ›ci AI w ulubionym stacku czy jÄ™zyku programowania.

Na koniec uczulamy jednak na fakt, Å¼e czym innym jest testowanie modelu w sposÃ³b bezpoÅ›redni, a czym innym jego wykorzystywanie np. poprzez Cursora czy Copilota. Warstwy poÅ›rednie wprowadzane przez dostawcÃ³w edytorÃ³w mogÄ… zmieniaÄ‡ dziaÅ‚anie danego LLMa, czego nie bÄ™dziemy w stanie odtworzyÄ‡ na poziomie testÃ³w. WÅ‚aÅ›nie dlatego kluczowe jest **Å‚Ä…czenie codziennej praktyki z toolingiem do testowania** surowej formy modeli - obie aktywnoÅ›ci dadzÄ… nam peÅ‚ny obraz tego, jak i kiedy zmieniaÄ‡ swoje nawyki wspÃ³Å‚pracy z AI.

## ğŸ‘¨â€ğŸ’» Ä†wiczenia praktyczne

### **Zadanie 1: Wykonaj pierwszy test promptfoo**

**Cel:** Przetestuj 3 modele - GPT-4o, GPT-5 oraz Gemini 2.5 Pro na wybranym prompcie

**Instrukcje:**

1. Zainicjalizuj promptfoo w konfiguracji pod zewnÄ™trzne pliki (prompt + testy + asercje)
2. Zdefiniuj scenariusz testowy dla 3 modeli (np. generowanie kodu TypeScript)
3. WprowadÅº test i jednÄ… uÅ¼ytecznÄ… asercjÄ™ (np. wyszukiwanie typÃ³w, uÅ¼ywanie kompilatora tsc)
4. Wykonaj test i sprawdÅº jakoÅ›Ä‡ modeli - podziel siÄ™ wynikami na [#Dyskusje - praktyka \[10X\]](https://bravecourses.circle.so/c/watki-dotyczace-lekcji-i-cwiczen)

Powodzenia!

![Ilustracja z lekcji](https://assets-v2.circle.so/ekrtcr8j44qd531ut1i2iwox5c9h)