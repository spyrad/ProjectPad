---
title: "[5x1] Poszerzanie wiedzy modelu - LLMs.txt"
course: "10xdevs-2"
source: "Przeprogramowani.pl"
exported: "2025-11-09"
format: "markdown"
---

## 

![Ilustracja z lekcji](https://assets-v2.circle.so/1fx5feeqoa74qditomizprh73c53)

## Wprowadzenie

Przy programowaniu z LLM coraz wiÄ™kszym wyzwaniem staje siÄ™ dostarczanie modelom aktualnej, dokÅ‚adnej wiedzy. ChoÄ‡ wspÃ³Å‚czesne LLM imponujÄ… swoimi moÅ¼liwoÅ›ciami, borykajÄ… siÄ™ z problemem nieaktualnych informacji, szczegÃ³lnie w szybko rozwijajÄ…cych siÄ™ dziedzinach takich jak web development.

W tej lekcji poznamy mechanizm **in-context learning** \- zdolnoÅ›Ä‡ modeli AI do tymczasowego uczenia siÄ™ z dostarczonego kontekstu bez potrzeby ponownego trenowania. Odkryjemy, jak przekazywaÄ‡ modelom aktualne informacje, aby generowaÅ‚y dokÅ‚adniejszy i bardziej uÅ¼yteczny kod.

SzczegÃ³lnÄ… uwagÄ™ poÅ›wiÄ™cimy standardowi **llms.txt** zaproponowanemu przez Jeremy'ego Howarda - rozwiÄ…zaniu analogicznemu do robots.txt, lecz skierowanemu do modeli AI. Ten prosty plik w formacie Markdown stanowi skondensowane ÅºrÃ³dÅ‚o wiedzy eksperckiej, uÅ‚atwiajÄ…c modelom dotarcie do aktualnej dokumentacji projektÃ³w i bibliotek.

Wszystko to w celu usprawnienia codziennej pracy programistycznej i maksymalizacji korzyÅ›ci pÅ‚ynÄ…cych z wykorzystania narzÄ™dzi AI w procesie wytwarzania oprogramowania.

## Nauka modelu za pomocÄ… kontekstu 

**In-context learning** to mechanizm, dziÄ™ki ktÃ³remu duÅ¼e modele jÄ™zykowe (LLM) potrafiÄ… dostosowaÄ‡ siÄ™ do nowych zadaÅ„ lub informacji na podstawie dostarczonego kontekstu, bez potrzeby ponownego trenowania modelu. 

W praktyce oznacza to, Å¼e model otrzymuje w prompcie dodatkowe dane â€“ np. opis zadania, reguÅ‚y czy nawet przykÅ‚ady â€“ i **tymczasowo â€uczy siÄ™â€ na ich podstawie** podczas generowania odpowiedzi. Wszystko dzieje siÄ™ wyÅ‚Ä…cznie w trakcie wnioskowania (inference), a po zakoÅ„czeniu konwersacji model nie zachowuje na staÅ‚e tej wiedzy. 

Innymi sÅ‚owy, wiedza zdobyta w ten sposÃ³b jest ulotna â€“ model zapomina jÄ… po zakoÅ„czeniu konwersacji. Mimo to, odpowiednio podany kontekst potrafi znaczÄ…co wpÅ‚ynÄ…Ä‡ na jakoÅ›Ä‡ odpowiedzi modelu i pozwala mu wyjÅ›Ä‡ poza ograniczenia wÅ‚asnego treningu.

W tym kursie wykorzystywaliÅ›my wielokrotnie in-context learning na rÃ³Å¼ne sposoby. Podstawowym byÅ‚o tworzenie **dokumentÃ³w planistycznych** â€“ plikÃ³w tekstowych opisujÄ…cych wymagania, architekturÄ™ lub plan dziaÅ‚ania dla danego zadania (prd.md, db-plan.md, endpoint-implementation-plan.md)

Tego typu â€pomoce naukoweâ€ dla AI sÄ… formÄ… in-context learning, bo model podÄ…Å¼a za dostarczonymi instrukcjami zamiast polegaÄ‡ wyÅ‚Ä…cznie na wÅ‚asnej (niekoniecznie aktualnej) wiedzy.

InnÄ… formÄ… in-context learning sÄ… **â€Rules for AIâ€** (reguÅ‚y dla AI), czyli plikÃ³w z reguÅ‚ami, ktÃ³re sÄ… doÅ‚Ä…czane do kontekstu rozmowy. W praktyce sÄ… to po prostu dodatkowe instrukcje, ktÃ³re model traktuje je jako wytyczne podczas generowania kodu. 

ReguÅ‚y i dokumenty planistyczne dziaÅ‚ajÄ… na podobnej zasadzie â€“ to **my uczymy model w kontekÅ›cie bieÅ¼Ä…cej sesji**, zamiast polegaÄ‡ jedynie na tym, co zostaÅ‚o w niego â€wbudowaneâ€ podczas trenowania.

Dlaczego to takie waÅ¼ne? OtÃ³Å¼ nawet najnowoczeÅ›niejsze modele majÄ… ograniczenia, jeÅ›li chodzi o aktualnoÅ›Ä‡ wiedzy a tym bardziej zrozumienie potrzeb wybranej firmy, zespoÅ‚u czy programisty.

LLMy sÄ… trenowane na ogromnych zbiorach danych, ale **sÄ… to dane historyczne** â€“ np. do 2023/2024 roku. JeÅ¼eli korzystamy z najnowszych bibliotek (Next.js 15, Tailwind 4 itp.), model czÄ™sto â€halucynujeâ€ nieistniejÄ…ce API albo generuje niepoprawny kod. 

Dla web developera oznacza to, Å¼e **asystent AI bez dodatkowej wiedzy moÅ¼e w niektÃ³rych zadaniach wnosiÄ‡ wiÄ™cej szkody niÅ¼ poÅ¼ytku.** Tutaj raz jeszcze musimy skorzystaÄ‡ z mechanizmu in-context learning.

PrzykÅ‚adem moÅ¼e byÄ‡ przekazanie fragmentÃ³w oficjalnej dokumentacji Next.js do konwersacji z Cursorem. WiÄ™kszoÅ›Ä‡ modeli ma ograniczonÄ… wiedzÄ™ o technologiach wydanych w drugiej poÅ‚owie 2024 roku, wiÄ™c moÅ¼e nie znaÄ‡ szczegÃ³Å‚Ã³w Next.js 15\. MoÅ¼emy jednak skopiowaÄ‡ z dokumentacji opis danego mechanizmu i dodaÄ‡ go do konwersacji z agentem . W ten sposÃ³b model â€przeczytaâ€ ten fragment i uÅ¼yje go podczas generowania sugestii. 

Takie podejÅ›cie jest skuteczne, choÄ‡ ma swoje ograniczenia:

- rÄ™czne kopiowanie dokumentacji bywa Å¼mudne,
- kopiujÄ…c caÅ‚oÅ›Ä‡ Å‚atwo przekroczyÄ‡ okno kontekstowe,
- model moÅ¼e nie uchwyciÄ‡ szerszego obrazu jeÅ›li dostarczymy mu wyrywkowe dane
- model moÅ¼e gubiÄ‡ informacje o szczegÃ³Å‚ach, jeÅ¼eli przekaÅ¼emy mu za duÅ¼o informacji

Niemniej jednak, przekazywanie dokumentacji do kontekstu zwykle potrafi ukierunkowaÄ‡ model na poprawne tory i zapobiec halucynacjom nieistniejÄ…cych funkcji.

## llms.txt â€“ propozycja standardu od Jeremy Howarda

Kolejnym aspektem problemu nieaktualnej wiedzy modeli jest standaryzacja sposobu, w jaki dostarczamy im aktualne informacje. Tutaj na scenÄ™ wchodzi plik **llms.txt** â€“ zaproponowany we wrzeÅ›niu 2024 r. standard autorstwa prof. Jeremyâ€™ego Howarda ([llmstxt.org](https://llmstxt.org/))

![Ilustracja z lekcji](https://assets-v2.circle.so/pzbahggxim2zor8et5x5i0q8sc5y)

Stoi za nim prosta idea: skoro roboty sieciowe majÄ… robots.txt i mapy witryn (sitemaps) do nawigacji po stronach, to stwÃ³rzmy analogiczny mechanizm dla modeli AI, aby **Å‚atwo przekazaÄ‡ im skondensowanÄ… wiedzÄ™ o zawartoÅ›ci strony lub projektu**.

**Czym jest llms.txt?** W duÅ¼ym skrÃ³cie, to plik w formacie Markdown, umieszczany w gÅ‚Ã³wnym katalogu strony (pod URL /llms.txt), zawierajÄ…cy przyjazne dla AI streszczenie i spis treÅ›ci dokumentacji danej strony czy projektu. Jeremy Howard opisuje go jako **â€LLM-friendly contentâ€** â€“ zwiÄ™zÅ‚e, eksperckie informacje zebrane w jednym miejscu, **czytelne zarÃ³wno dla czÅ‚owieka, jak i dla modelu**. 

![Ilustracja z lekcji](https://assets-v2.circle.so/a7olgbmetaz2v3kvgumzgjk561gs)

 _PrzykÅ‚ad llms.txt dla strony_ [_astro.build/llms.txt_](https://astro.build/llms.txt)

Taki plik zwykle zawiera krÃ³tkie tÅ‚o projektu, waÅ¼ne wskazÃ³wki oraz listÄ™ odnoÅ›nikÃ³w do szczegÃ³Å‚owych plikÃ³w (np. dokumentacji) â€“ przy czym w idealnym scenariuszu te szczegÃ³Å‚owe strony rÃ³wnieÅ¼ majÄ… swoje wersje .md do Å‚atwego przetworzenia przez AI. 

W efekcie model otrzymuje coÅ› w rodzaju **skompresowanej dokumentacji**, ktÃ³rÄ… moÅ¼e szybko wczytaÄ‡ w oknie kontekstu, zamiast prÃ³bowaÄ‡ przeszukiwaÄ‡ setki podstron peÅ‚nych nawigacji, reklam czy zbÄ™dnego szumu ze standardowych stron HTML.

Standard **llms.txt** ma na celu **uÅ‚atwiÄ‡ modelom dotarcie do aktualnej wiedzy**. Zamiast liczyÄ‡, Å¼e model sam â€wygrzebieâ€ potrzebne informacje z internetu za pomocÄ… wyszukiwarki @Web, moÅ¼emy mu podaÄ‡ gotowe podsumowanie i Å›cieÅ¼ki do ÅºrÃ³deÅ‚. 

RozwiÄ…zuje to czÄ™Å›ciowo problem **fragmentacji ÅºrÃ³deÅ‚ wiedzy** â€“ obecnie dokumentacja bywa rozproszona (osobne strony dla kaÅ¼dej funkcji, blogi z poradami, wiki spoÅ‚ecznoÅ›ci itp.), co utrudnia AI znalezienie konkretnej informacji w trakcie pojedynczej sesji. DziÄ™ki llms.txt autor strony czy biblioteki moÅ¼e **zunifikowaÄ‡ najwaÅ¼niejsze informacje w jednym pliku**, wskazujÄ…c jednoczeÅ›nie, gdzie szukaÄ‡ szczegÃ³Å‚Ã³w. To znacznie zmniejsza obciÄ…Å¼enie modelu â€“ nie musi crawlowaÄ‡ caÅ‚ej witryny, wystarczy Å¼e przeczyta przygotowane podsumowanie. 

Dobrze opracowany plik llms.txt **standaryzuje format** takich podsumowaÅ„, co oznacza, Å¼e rÃ³Å¼ne narzÄ™dzia AI mogÄ… go przetwarzaÄ‡ w zautomatyzowany sposÃ³b (np. parsowaÄ‡ po nagÅ‚Ã³wkach, czytaÄ‡ listÄ™ plikÃ³w).

Co waÅ¼ne, llms.txt **nie zastÄ™puje peÅ‚nej dokumentacji**, a raczej jÄ… uzupeÅ‚nia. Zwykle to plik wzglÄ™dnie krÃ³tki, zawierajÄ…cy opisy i linki, ale bez caÅ‚ego kodu ÅºrÃ³dÅ‚owego dokumentacji. Dlatego czÄ™sto towarzyszy mu drugi plik â€“ **llms-full.txt** â€“ ktÃ³ry zawiera juÅ¼ _peÅ‚nÄ…_ treÅ›Ä‡ dokumentacji w jednym pliku (flattened content). RÃ³Å¼nica jest taka, Å¼e llms.txt to **spis treÅ›ci z opisami**, wymagajÄ…cy od narzÄ™dzia AI podÄ…Å¼ania za linkami, natomiast llms-full.txt to **wszystko w jednym**. Ten drugi bywa bardzo duÅ¼e (setki tysiÄ™cy tokenÃ³w, przez co zwykle nie zmieÅ›ci siÄ™ w kontekÅ›cie modelu. Jak sobie z tym poradziÄ‡? O tym w kolejnej sekcji.

NiezaleÅ¼nie od podejÅ›cia, standaryzacja jest kluczowa â€“ ujednolicony format oznacza, Å¼e narzÄ™dzia deweloperskie mogÄ… automatycznie rozpoznawaÄ‡ i wykorzystywaÄ‡ llms.txt, jeÅ›li jest dostÄ™pny.

W ciÄ…gu ostatnich miesiÄ™cy coraz wiÄ™cej projektÃ³w eksperymentuje z tym standardem. PrzykÅ‚adowo, Stripe udostÄ™pniÅ‚o plik llms.txt dla swojej dokumentacji API, a spoÅ‚ecznoÅ›Ä‡ Angulara zaproponowaÅ‚a dodanie takich plikÃ³w do oficjalnej strony Angular.dev ([Github Issue](https://github.com/angular/angular/issues/60434) autorstwa kursanta 10xDevs, PawÅ‚a - pozdro!)

MoÅ¼na wiÄ™c przypuszczaÄ‡, Å¼e z czasem llms.txt stanie siÄ™ tak oczywisty, jak dziÅ› robots.txt â€“ zwÅ‚aszcza jeÅ›li IDE i edytory zacznÄ… natywnie go obsÅ‚ugiwaÄ‡. 

## ğŸ Podsumowanie

W tej lekcji poznaliÅ›my mechanizmy uzupeÅ‚niania wiedzy modeli jÄ™zykowych oraz standaryzacji przekazywania im aktualnych informacji:

- **In-context learning** \- mechanizm pozwalajÄ…cy modelom jÄ™zykowym dostosowaÄ‡ siÄ™ do nowych zadaÅ„ lub informacji bez ponownego trenowania, poprzez dostarczenie kontekstu w trakcie wnioskowania (dokumenty planistyczne, reguÅ‚y dla AI).
- **Problem nieaktualnej wiedzy** \- nawet najnowoczeÅ›niejsze modele majÄ… ograniczenia dotyczÄ…ce aktualnoÅ›ci wiedzy, zwÅ‚aszcza o nowych technologiach, co prowadzi do "halucynowania" nieistniejÄ…cych API lub generowania niepoprawnego kodu.
- **Standard llms.txt** \- zaproponowany przez Jeremy'ego Howarda format pliku Markdown umieszczanego w gÅ‚Ã³wnym katalogu strony, zawierajÄ…cy przyjazne dla AI streszczenie i spis treÅ›ci dokumentacji, uÅ‚atwiajÄ…cy modelom dotarcie do aktualnej wiedzy.
- **Tryby konsumowania wiedzy** \- moÅ¼liwoÅ›Ä‡ rÄ™cznego wyszukiwania potrzebnych fragmentÃ³w dokumentacji lub automatycznego wstrzykiwania przez MCP

PamiÄ™taj, Å¼e wymienione techniki nie wykluczajÄ… siÄ™ wzajemnie - najlepsze efekty osiÄ…gniesz Å‚Ä…czÄ…c je. DziÄ™ki wykorzystaniu mechanizmÃ³w in-context learning, standardu llms.txt oraz narzÄ™dzi takich jak Context7, moÅ¼esz znaczÄ…co poprawiÄ‡ jakoÅ›Ä‡ generowanego kodu i zredukowaÄ‡ problemy zwiÄ…zane z nieaktualnymi wzorcami.

### **ğŸ‘¨â€ğŸ’» Ä†wiczenia praktyczne**

> ğŸ‘‰ JeÅ›li pracujesz nad projektem zaliczeniowym, potraktuj poniÅ¼sze Ä‡wiczenie jako opcjonalne - podejdÅº do niego w momencie, kiedy znajdziesz wiÄ™cej czasu.

**Zadanie 1: PorÃ³wnanie skutecznoÅ›ci rÃ³Å¼nych metod dostarczania kontekstu**

**Cel**: Analiza efektywnoÅ›ci rÃ³Å¼nych podejÅ›Ä‡ do in-context learning.

**Instrukcje**:

1. Wybierz konkretne zadanie programistyczne wymagajÄ…ce znajomoÅ›ci najnowszej wersji frameworka/biblioteki
2. Przetestuj nastÄ™pujÄ…ce metody dostarczania kontekstu:  
   - BezpoÅ›rednie zapytanie bez dodatkowego kontekstu (baseline)  
   - RÄ™czne skopiowanie fragmentu dokumentacji z oficjalnej strony  
   - Wykorzystanie Context7 do wyszukania odpowiedniego fragmentu  
   - UÅ¼ycie peÅ‚nego pliku llms.txt (jeÅ›li dostÄ™pny dla danej technologii)
3. Dla kaÅ¼dej metody wykonaj to samo zadanie, korzystajÄ…c z tego samego modelu AI
4. OceniÄ‡ wyniki pod kÄ…tem:  
   - PoprawnoÅ›ci wygenerowanego kodu  
   - Liczby halucynacji lub bÅ‚Ä™dnych sugestii  
   - Czasu potrzebnego na uzyskanie poprawnego rozwiÄ…zania  
   - ObjÄ™toÅ›ci kontekstu (liczby tokenÃ³w)
5. Udokumentuj wnioski, okreÅ›lajÄ…c najefektywniejszÄ… metodÄ™ dla wybranego zadania

**WaÅ¼ne**: W zadaniach moÅ¼esz wykorzystaÄ‡ rÃ³Å¼ne modele AI (Claude, GPT, Gemini), aby porÃ³wnaÄ‡ jak rÃ³Å¼ne modele reagujÄ… na dostarczony kontekst. Celem jest nie tylko wykonanie zadaÅ„, ale takÅ¼e zrozumienie, jak efektywnie wykorzystywaÄ‡ mechanizm in-context learning w codziennej pracy.

![Ilustracja z lekcji](https://assets-v2.circle.so/ekrtcr8j44qd531ut1i2iwox5c9h)